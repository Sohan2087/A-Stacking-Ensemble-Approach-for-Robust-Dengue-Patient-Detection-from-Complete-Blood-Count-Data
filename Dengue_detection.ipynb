{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AS9IFBlPnrk-",
    "outputId": "e691c487-80b6-407e-c5c4-2ca32f91159c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.2.0)\n",
      "Installing collected packages: imblearn\n",
      "Successfully installed imblearn-0.0\n",
      "Collecting scikit-learn==1.1.3\n",
      "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (3.2.0)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 0.12.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.1.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "sklearn"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting missingpy\n",
      "  Downloading missingpy-0.2.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: missingpy\n",
      "Successfully installed missingpy-0.2.0\n",
      "Collecting sweetviz\n",
      "  Downloading sweetviz-2.2.1-py3-none-any.whl (15.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.23.5)\n",
      "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (3.7.1)\n",
      "Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (4.66.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.11.3)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (3.1.2)\n",
      "Requirement already satisfied: importlib-resources>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (6.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.1->sweetviz) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.3->sweetviz) (1.16.0)\n",
      "Installing collected packages: sweetviz\n",
      "Successfully installed sweetviz-2.2.1\n",
      "Collecting classification\n",
      "  Downloading classification-0.1.0-py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: classification\n",
      "Successfully installed classification-0.1.0\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.3)\n",
      "Collecting catboost\n",
      "  Downloading catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.3)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
      "Installing collected packages: catboost\n",
      "Successfully installed catboost-1.2.2\n",
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.23.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.11.3)\n",
      "Collecting sklearn-genetic\n",
      "  Downloading sklearn_genetic-0.5.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in /usr/local/lib/python3.10/dist-packages (from sklearn-genetic) (1.1.3)\n",
      "Collecting deap>=1.0.2 (from sklearn-genetic)\n",
      "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sklearn-genetic) (1.23.5)\n",
      "Collecting multiprocess (from sklearn-genetic)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23->sklearn-genetic) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23->sklearn-genetic) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23->sklearn-genetic) (3.2.0)\n",
      "Collecting dill>=0.3.7 (from multiprocess->sklearn-genetic)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: dill, deap, multiprocess, sklearn-genetic\n",
      "Successfully installed deap-1.4.1 dill-0.3.7 multiprocess-0.70.15 sklearn-genetic-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn\n",
    "!pip install scikit-learn==1.1.3\n",
    "!pip install missingpy\n",
    "!pip install sweetviz\n",
    "!pip install classification\n",
    "\n",
    "!pip install xgboost\n",
    "!pip install catboost\n",
    "!pip install lightgbm\n",
    "!pip install sklearn-genetic\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDDDQDM7nyz_"
   },
   "source": [
    "# Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3q36huHnz9f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "# from utils import is_number\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#Impute Libraries\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer as MICE\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "#Import SVM\n",
    "from sklearn.svm import SVC\n",
    "#Import library for logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score,accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from math import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score,accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier #RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_score,accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "from math import *\n",
    "\n",
    "\n",
    "import sys\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib as mpl\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import catboost as cb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, RidgeClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etqthQNZr-qC"
   },
   "outputs": [],
   "source": [
    "raw_data_frame = pd.read_csv('processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkKdaNhVsc44"
   },
   "outputs": [],
   "source": [
    "print(raw_data_frame['Sex'].value_counts())\n",
    "raw_data_frame['Sex'].replace(' ', np.NaN, inplace = True)\n",
    "raw_data_frame['Sex'].value_counts()\n",
    "raw_data_frame['Sex'] = raw_data_frame['Sex'].replace({'M': 1, 'F':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9HpWSFTsnnA"
   },
   "source": [
    "# Missing data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BH_lpTXSsd94"
   },
   "outputs": [],
   "source": [
    "def missing_imputaion(x,imputer='none'):\n",
    "    xt=x\n",
    "    if imputer=='knn':\n",
    "\n",
    "        X = xt\n",
    "        imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "        Knn_data=imputer.fit_transform(X)\n",
    "        X1=pd.DataFrame(Knn_data)\n",
    "        y1=list(xt.columns.values)\n",
    "        X1.columns=y1\n",
    "        return X1\n",
    "    elif imputer=='mice':\n",
    "        Mice_data=MICE().fit_transform(xt)\n",
    "        X1=pd.DataFrame(Mice_data)\n",
    "        y1=list(xt.columns.values)\n",
    "        X1.columns=y1\n",
    "        return X1\n",
    "    elif imputer=='randomforest':\n",
    "        imputer = MissForest()\n",
    "        Rf = imputer.fit_transform(xt)\n",
    "        X1=pd.DataFrame(Rf)\n",
    "        y1=list(xt.columns.values)\n",
    "        X1.columns=y1\n",
    "        return X1\n",
    "    else:\n",
    "        X1=xt.dropna(axis=0)\n",
    "        return(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rixECSuYstPZ"
   },
   "outputs": [],
   "source": [
    "labels = raw_data_frame['Dengue_NS1']\n",
    "data_raw = raw_data_frame.drop(['Dengue_NS1'],1)\n",
    "\n",
    "data_raw = missing_imputaion(x=data_raw, imputer='mice')\n",
    "\n",
    "import re\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "data_raw.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in data_raw.columns.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt7dQc1rtDTA"
   },
   "source": [
    "# Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8l_3gOYKs1oZ"
   },
   "outputs": [],
   "source": [
    "columns = list(data_raw.columns)\n",
    "for column in columns:\n",
    "      data_raw[column] = (data_raw[column] - data_raw[column].mean()) / (data_raw[column].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fAwzmESthTJ"
   },
   "source": [
    "#Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFH_f8eltWjK"
   },
   "outputs": [],
   "source": [
    "def feature_selection_without_gen(x,y):\n",
    "        X_train=x\n",
    "        y_train=y\n",
    "        mdl=[]\n",
    "        mdl.append(xgb.XGBClassifier(\n",
    "                        max_depth=4\n",
    "                        ,learning_rate=0.2\n",
    "                        ,reg_lambda=1\n",
    "                        ,n_estimators=150\n",
    "                        ,subsample = 0.9\n",
    "                        ,colsample_bytree = 0.9))\n",
    "        mdl.append(RandomForestClassifier(n_estimators=50,max_depth=10,\n",
    "                                            random_state=0,class_weight=None,\n",
    "                                            n_jobs=-1))\n",
    "        mdl.append(ExtraTreesClassifier())\n",
    "        ml1=['XGBoost','Random_Forest','Extra_Tree']\n",
    "        feat_sel=[]\n",
    "        for i in range(3):\n",
    "\n",
    "            model=mdl[i]\n",
    "            model.fit(X_train, y_train)\n",
    "            model.feature_importances_\n",
    "            importances = model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            feat_labels = X_train.columns\n",
    "            print(\"Feature ranking:\")\n",
    "            sel_feat=[]\n",
    "            for f in range(X_train.shape[1]):\n",
    "                    print(\"%d. feature no:%d feature name:%s (%f)\" % (f+1, indices[f], feat_labels[indices[f]], importances[indices[f]]))\n",
    "                    sel_feat.append(feat_labels[indices[f]])\n",
    "            top_n=20\n",
    "            feat_sel.append(sel_feat)\n",
    "            indices = indices[0:top_n]\n",
    "            plt.subplots(figsize=(12, 10))\n",
    "            g = sns.barplot(x= importances[indices], y = feat_labels[indices], orient='h',label='big') #import_feature.iloc[:Num_f]['col'].values[indices]\n",
    "\n",
    "            g.set_title(ml1[i]+' feature selection',fontsize=25)\n",
    "            g.set_xlabel(\"Relative importance\",fontsize=25)\n",
    "            g.set_ylabel(\"Features\",fontsize=25)\n",
    "            g.tick_params(labelsize=14)\n",
    "            sns.despine()\n",
    "                # plt.savefig('feature_importances_v3.png')\n",
    "            plt.show()\n",
    "            print('-----------------------------------------------------------------')\n",
    "        xgboost=feat_sel[0]\n",
    "        randomforest=feat_sel[1]\n",
    "        extratree=feat_sel[2]\n",
    "\n",
    "        return(xgboost,randomforest,extratree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nawWBhLptphy"
   },
   "outputs": [],
   "source": [
    "xgboost,randomforest,extratree = feature_selection_without_gen(data_raw, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1w4LhtWMuBYx"
   },
   "source": [
    "# Create folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-oGF6ERuFwb"
   },
   "outputs": [],
   "source": [
    "def cv_fold(X1,yt,n_splits=5,shuffle=False):\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    smote=SMOTE()\n",
    "    cc=X1.columns\n",
    "    xx1 = StandardScaler().fit_transform(X1)\n",
    "    X=np.array(xx1)\n",
    "    y=np.array(yt)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=None, shuffle=shuffle)\n",
    "    skf.get_n_splits(X, y)\n",
    "    xtrain=[]\n",
    "    xtest=[]\n",
    "    ytrain=[]\n",
    "    ytest=[]\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            x1,y1= smote.fit_resample(X_train, y_train)\n",
    "            xtrain.append(x1)\n",
    "            xtest.append(X_test)\n",
    "            ytrain.append(y1)\n",
    "            ytest.append(y_test)\n",
    "    d={'data':(xtrain,xtest,ytrain,ytest),'index':cc}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiINMUwUuMDZ"
   },
   "outputs": [],
   "source": [
    "fold_data = cv_fold(X1=data_raw,yt=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhH3sch9uQtg"
   },
   "source": [
    "# Train with top features\n",
    "Model Index:\n",
    "\n",
    "'MLPClassifier' 0,\n",
    "\n",
    "'LinearDiscriminantAnalysis' 1,\n",
    "\n",
    "'XGBClassifier' 2,\n",
    "\n",
    "'RandomForestClassifier' 3,\n",
    "\n",
    "'LogisticRegression' 4,\n",
    "\n",
    "'SVM' 5,\n",
    "\n",
    "'ExtraTreesClassifier' 6,\n",
    "\n",
    "'AdaBoostClassifier' 7,\n",
    "\n",
    "'KNeighborsClassifier' 8,\n",
    "\n",
    "'GradientBoostingClassifier' 9,\n",
    "\n",
    "'XGB_untuned' 10,\n",
    "\n",
    "'CatBoost_untuned' 11,\n",
    "\n",
    "'LGBM_untuned' 12,\n",
    "\n",
    "'AdaBoost_untuned' 13,\n",
    "\n",
    "'SVC_untuned' 14,\n",
    "\n",
    "'RandomForest_untuned' 15,\n",
    "\n",
    "'ExtraTrees_untuned' 16,\n",
    "\n",
    "'KNeighbors_untuned' 17,\n",
    "\n",
    "'LDA_untuned' 18,\n",
    "\n",
    "'LogisticRegression_untuned' 19,\n",
    "\n",
    "'ElasticNet_untuned' 20,\n",
    "\n",
    "'Ridge Classifier' 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtPkrFbwuVkA"
   },
   "outputs": [],
   "source": [
    "def models():\n",
    "\n",
    "        clf=[]\n",
    "        MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "               hidden_layer_sizes=(13, 13), learning_rate='constant',\n",
    "               learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
    "               nesterovs_momentum=True, power_t=0.5, random_state=111,\n",
    "               shuffle=False, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "               verbose=False, warm_start=False)\n",
    "        clf.append(MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500))\n",
    "\n",
    "\n",
    "        clf.append(LinearDiscriminantAnalysis())\n",
    "\n",
    "        clf.append(xgb.XGBClassifier(\n",
    "                        max_depth=85\n",
    "                        ,learning_rate=0.9388440565186442,\n",
    "                        min_split_loss= 0.0\n",
    "                        ,reg_lambda=5.935581318908179\n",
    "                        ,min_child_weight= 2.769401581888831\n",
    "                        ,colsample_bylevel= 0.7878344729848824\n",
    "                        ,colsample_bynode=0.4895496034538383\n",
    "                        ,alpha= 7.9692927383000445\n",
    "                        ,n_estimators=150\n",
    "                        ,subsample = 0.2656532818978606\n",
    "                        ,colsample_bytree = 0.8365485367400313))\n",
    "\n",
    "        clf.append(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                               max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
    "                               min_impurity_decrease=0.0,\n",
    "                               min_samples_leaf=1, min_samples_split=2,\n",
    "                               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                               n_jobs=None, oob_score=False, random_state=0,\n",
    "                               verbose=0, warm_start=False))\n",
    "\n",
    "\n",
    "        clf.append(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                           intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                           multi_class='multinomial', n_jobs=None, penalty='l2',\n",
    "                           random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                           warm_start=False))\n",
    "\n",
    "\n",
    "        clf.append(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                decision_function_shape='ovr', degree=3, gamma='auto',\n",
    "                kernel='linear', max_iter=100, probability=True, random_state=0,\n",
    "                shrinking=True, tol=0.001, verbose=False))\n",
    "\n",
    "\n",
    "        clf.append(ExtraTreesClassifier(n_estimators=100, max_depth=8, min_samples_split=10, random_state=0))\n",
    "\n",
    "        clf.append(AdaBoostClassifier(n_estimators=100, random_state=0))\n",
    "\n",
    "        clf.append(KNeighborsClassifier(n_neighbors=3))\n",
    "        clf.append(GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=10, random_state=0))\n",
    "\n",
    "        clf.append(XGBClassifier(n_estimators=400,\n",
    "                      iterations=500,\n",
    "                      learning_rate=0.001,\n",
    "                      loss_function='Logloss'))\n",
    "        clf.append(cb.CatBoostClassifier())\n",
    "        clf.append(LGBMClassifier(learning_rate=0.01))\n",
    "        clf.append(AdaBoostClassifier(learning_rate=0.001))\n",
    "        clf.append(SVC(probability=True))\n",
    "        clf.append(RandomForestClassifier())\n",
    "        clf.append(ExtraTreesClassifier(bootstrap=True))\n",
    "        clf.append(KNeighborsClassifier(n_neighbors=3))\n",
    "        clf.append(LinearDiscriminantAnalysis())\n",
    "        clf.append(LogisticRegression())\n",
    "        clf.append(LogisticRegression(penalty='elasticnet', l1_ratio=0.01, solver='saga'))\n",
    "        #clf.append(RidgeClassifier())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        clff=['MLPClassifier','LinearDiscriminantAnalysis','XGBClassifier','RandomForestClassifier','LogisticRegression','SVM','ExtraTreesClassifier','AdaBoostClassifier','KNeighborsClassifier','GradientBoostingClassifier','XGB_untuned', 'CatBoost_untuned', 'LGBM_untuned', 'AdaBoost_untuned', 'SVC_untuned', 'RandomForest_untuned', 'ExtraTrees_untuned', 'KNeighbors_untuned', 'LDA_untuned', 'LogisticRegression_untuned', 'ElasticNet_untuned']\n",
    "        #, 'Ridge_untuned'\n",
    "        #Result.to_csv\n",
    "        return(clf,clff )\n",
    "\n",
    "\n",
    "def models_v_2():\n",
    "\n",
    "        clf=[]\n",
    "        MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "               hidden_layer_sizes=(13, 13), learning_rate='constant',\n",
    "               learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
    "               nesterovs_momentum=True, power_t=0.5, random_state=111,\n",
    "               shuffle=False, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "               verbose=False, warm_start=False)\n",
    "        clf.append(MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500))\n",
    "\n",
    "\n",
    "        clf.append(LinearDiscriminantAnalysis())\n",
    "\n",
    "        clf.append(xgb.XGBClassifier(\n",
    "                        max_depth=85\n",
    "                        ,learning_rate=0.9388440565186442,\n",
    "                        min_split_loss= 0.0\n",
    "                        ,reg_lambda=5.935581318908179\n",
    "                        ,min_child_weight= 2.769401581888831\n",
    "                        ,colsample_bylevel= 0.7878344729848824\n",
    "                        ,colsample_bynode=0.4895496034538383\n",
    "                        ,alpha= 7.9692927383000445\n",
    "                        ,n_estimators=150\n",
    "                        ,subsample = 0.2656532818978606\n",
    "                        ,colsample_bytree = 0.8365485367400313))\n",
    "\n",
    "        clf.append(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                               max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
    "                               min_impurity_decrease=0.0,\n",
    "                               min_samples_leaf=1, min_samples_split=2,\n",
    "                               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                               n_jobs=None, oob_score=False, random_state=0,\n",
    "                               verbose=0, warm_start=False))\n",
    "\n",
    "\n",
    "        clf.append(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                           intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                           multi_class='multinomial', n_jobs=None, penalty='l2',\n",
    "                           random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                           warm_start=False))\n",
    "\n",
    "\n",
    "        clf.append(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                decision_function_shape='ovr', degree=3, gamma='auto',\n",
    "                kernel='linear', max_iter=100, probability=True, random_state=0,\n",
    "                shrinking=True, tol=0.001, verbose=False))\n",
    "\n",
    "\n",
    "        clf.append(ExtraTreesClassifier(n_estimators=100, max_depth=8, min_samples_split=10, random_state=0))\n",
    "\n",
    "        clf.append(AdaBoostClassifier(n_estimators=100, random_state=0))\n",
    "\n",
    "        clf.append(KNeighborsClassifier(n_neighbors=3))\n",
    "        clf.append(GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=10, random_state=0))\n",
    "\n",
    "        clf.append(XGBClassifier(n_estimators=400,\n",
    "                      iterations=500,\n",
    "                      learning_rate=0.001,\n",
    "                      loss_function='Logloss'))\n",
    "        clf.append(cb.CatBoostClassifier())\n",
    "        clf.append(LGBMClassifier(learning_rate=0.01))\n",
    "        clf.append(AdaBoostClassifier(learning_rate=0.001))\n",
    "        clf.append(SVC(probability=True))\n",
    "        clf.append(RandomForestClassifier())\n",
    "        clf.append(ExtraTreesClassifier(bootstrap=True))\n",
    "        clf.append(KNeighborsClassifier(n_neighbors=3))\n",
    "        clf.append(LinearDiscriminantAnalysis())\n",
    "        clf.append(LogisticRegression())\n",
    "        clf.append(LogisticRegression(penalty='elasticnet', l1_ratio=0.01, solver='saga'))\n",
    "        clf.append(RidgeClassifier())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        clff=['MLPClassifier','LinearDiscriminantAnalysis','XGBClassifier','RandomForestClassifier','LogisticRegression','SVM','ExtraTreesClassifier','AdaBoostClassifier','KNeighborsClassifier','GradientBoostingClassifier','XGB_untuned', 'CatBoost_untuned', 'LGBM_untuned', 'AdaBoost_untuned', 'SVC_untuned', 'RandomForest_untuned', 'ExtraTrees_untuned', 'KNeighbors_untuned', 'LDA_untuned', 'LogisticRegression_untuned', 'ElasticNet_untuned', 'Ridge_untuned']\n",
    "\n",
    "        #\n",
    "        #Result.to_csv\n",
    "        return(clf,clff)\n",
    "\n",
    "def classification_with_top_feature_v_2(data,feature_num,feature_selection_model,classifier,feat_increment):\n",
    "\n",
    "        xtrain,xtest,ytrain,ytest=data['data']\n",
    "        ind=data['index'].to_list()\n",
    "        num_feat=feature_num\n",
    "        fsm=feature_selection_model\n",
    "        feature=fsm[0:num_feat]\n",
    "        clf,clff=models_v_2()\n",
    "\n",
    "\n",
    "        if classifier=='all':\n",
    "            l=0\n",
    "            for c in range(22):\n",
    "\n",
    "                clf1=clf[c]\n",
    "                a=[]\n",
    "                p=[]\n",
    "                r=[]\n",
    "                s=[]\n",
    "                f=[]\n",
    "                mean_tpr=[]\n",
    "                mean_auc=[]\n",
    "\n",
    "                feat=[]\n",
    "                for i in list(range(0,num_feat,feat_increment)):\n",
    "\n",
    "                    y_pred=[]\n",
    "                    y2=[]\n",
    "                    tl=fsm[0:i+1]  #feature increasing\n",
    "                    tprs = []\n",
    "                    aucs = []\n",
    "                    mean_fpr = np.linspace(0,1,100)\n",
    "\n",
    "                    total_fold_num = len(xtrain)\n",
    "                    for k in range(total_fold_num):\n",
    "                        x11=pd.DataFrame(xtrain[k])\n",
    "                        x11.columns=ind\n",
    "                        x1=x11[tl]\n",
    "                        y1=ytrain[k]\n",
    "                        model = clf1.fit(np.array(x1),np.array(y1))\n",
    "                        #model = clf1.fit(x[train],y.iloc[train])\n",
    "                        xts=pd.DataFrame(xtest[k])\n",
    "                        xts.columns=ind\n",
    "                        xt1=xts[tl]\n",
    "                        y_pr=model.predict(xt1)\n",
    "\n",
    "\n",
    "                        y_pred.extend(y_pr)\n",
    "                        y2.extend(ytest[k])\n",
    "\n",
    "\n",
    "\n",
    "                    y21=y2\n",
    "                    y_pred1=y_pred\n",
    "                    categories=list(pd.Series(y2).unique())\n",
    "\n",
    "\n",
    "\n",
    "                    from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "                    # main confusion matrix\n",
    "                    cm = confusion_matrix(y21, y_pred1)\n",
    "                    # cm_per_class: it returns a 2x2 confusion matrix for each class, where 'i' represnt  class index\n",
    "                    # cm_per_class[i][0][0]:TN,   cm_per_class[i][0][1]:FP,   cm_per_class[i][1][0]:FN,    cm_per_class[i][1][1]:TP\n",
    "                    cm_per_class = multilabel_confusion_matrix(y21, y_pred1)\n",
    "                    # Overall Accuracy\n",
    "                    Overall_Accuracy = np.sum(np.diagonal(cm)) / np.sum(cm)\n",
    "                    Overall_Accuracy = round(Overall_Accuracy*100, 2)\n",
    "                    # create confusion matrix table (pd.DataFrame)\n",
    "                    # cm_table = pd.DataFrame(cm, index=categories , columns=categories)\n",
    "                    if (i+1)!=1:\n",
    "                      feature_no= 'top_'+str(i+1)+'_features'\n",
    "                    else:\n",
    "                      feature_no= 'top_'+str(i+1)+'_feature'\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        Eval_Mat = []\n",
    "                        # per class metricies\n",
    "                        for i in range(len(categories)):\n",
    "                            TN = cm_per_class[i][0][0]\n",
    "                            FP = cm_per_class[i][0][1]\n",
    "                            FN = cm_per_class[i][1][0]\n",
    "                            TP = cm_per_class[i][1][1]\n",
    "                            Accuracy = round(100*(TP+TN)/(TP+TN+FP+FN), 2)\n",
    "                            Precision = round(100*(TP)/(TP+FP), 2)\n",
    "                            Sensitivity = round(100*(TP)/(TP+FN), 2)\n",
    "                            F1_score = round((2*Precision*Sensitivity)/(Precision+Sensitivity), 2)\n",
    "                            Specificity = round(100*(TN)/(TN+FP), 2)\n",
    "                            Eval_Mat.append([Accuracy, Precision, Sensitivity, F1_score, Specificity])\n",
    "                        # sizes of each class\n",
    "                        s2 = np.sum(cm,axis=1)\n",
    "                        # create tmep excel table\n",
    "                        headers=['Accuracy', 'Precision', 'Sensitivity', 'F1_score', 'Specificity']\n",
    "                        temp_table = pd.DataFrame(Eval_Mat, index=categories ,columns=headers)\n",
    "                        # weighted average of per class metricies\n",
    "                        ac=Overall_Accuracy\n",
    "                        # ac = round(temp_table['Accuracy'].dot(s)/np.sum(s), 2)\n",
    "                        pr = round(temp_table['Precision'].dot(s2)/np.sum(s2), 2)\n",
    "                        rc = round(temp_table['Sensitivity'].dot(s2)/np.sum(s2), 2)\n",
    "                        f1 = round(temp_table['F1_score'].dot(s2)/np.sum(s2), 2)\n",
    "                        sp = round(temp_table['Specificity'].dot(s2)/np.sum(s2), 2)\n",
    "\n",
    "\n",
    "                    except:\n",
    "                        ac='NaN'\n",
    "                        # ac = round(temp_table['Accuracy'].dot(s)/np.sum(s), 2)\n",
    "                        pr = 'NaN'\n",
    "                        rc ='NaN'\n",
    "                        f1 = 'NaN'\n",
    "                        sp = 'NaN'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    a.append(ac)\n",
    "                    p.append(pr)\n",
    "                    r.append(rc)\n",
    "                    s.append(sp)\n",
    "                    f.append(f1)\n",
    "                    feat.append(feature_no)\n",
    "\n",
    "\n",
    "\n",
    "                Result=pd.concat([pd.DataFrame(a),pd.DataFrame(p),pd.DataFrame(r),pd.DataFrame(s),pd.DataFrame(f)],1)\n",
    "                Result.columns=['Accuracy','Precision','Recall','Specificity','F1-score']\n",
    "                Result.index= feat\n",
    "                #Result.to_csv\n",
    "                print('---------------------------------------------------------------------')\n",
    "                print('Result for '+clff[l]+' classifier')\n",
    "                print('---------------------------------------------------------------------')\n",
    "                print(Result)\n",
    "    #             Result.to_csv('/content/'+clff[l]+'_classifier_for_top10_features.csv')\n",
    "\n",
    "                print('---------------------------------------------------------------------')\n",
    "                l=l+1\n",
    "\n",
    "            return\n",
    "        else:\n",
    "\n",
    "                clf1=clf[classifier]  #model 0= MLP, 1= LDA, 2 = XGBoost, 3 = RF, 4= Logit, 5=SVC, 6 = Extra tree, 7= Adaboost, 8 = KNN, 9 = GradientBoost\n",
    "                l=classifier\n",
    "#             l=0\n",
    "\n",
    "\n",
    "#                 clf1=clf[c]\n",
    "                a=[]\n",
    "                p=[]\n",
    "                r=[]\n",
    "                s=[]\n",
    "                f=[]\n",
    "                mean_tpr=[]\n",
    "                mean_auc=[]\n",
    "\n",
    "                feat=[]\n",
    "                for i in list(range(0,num_feat,feat_increment)):\n",
    "\n",
    "                    y_pred=[]\n",
    "                    y2=[]\n",
    "                    tl=fsm[0:i+1]\n",
    "                    tprs = []\n",
    "                    aucs = []\n",
    "                    mean_fpr = np.linspace(0,1,100)\n",
    "\n",
    "                    total_fold_num = len(xtrain)\n",
    "                    for k in range(total_fold_num):\n",
    "                        x11=pd.DataFrame(xtrain[k])\n",
    "                        x11.columns=ind\n",
    "                        x1=x11[tl]\n",
    "                        y1=ytrain[k]\n",
    "                        model = clf1.fit(np.array(x1),np.array(y1))\n",
    "                        #model = clf1.fit(x[train],y.iloc[train])\n",
    "                        xts=pd.DataFrame(xtest[k])\n",
    "                        xts.columns=ind\n",
    "                        xt1=xts[tl]\n",
    "                        y_pr=model.predict(np.array(xt1))\n",
    "\n",
    "                        y_pred.extend(y_pr)\n",
    "                        y2.extend(ytest[k])\n",
    "\n",
    "\n",
    "\n",
    "                    y21=y2\n",
    "                    y_pred1=y_pred\n",
    "\n",
    "                    categories=list(pd.Series(y2).unique())\n",
    "                    from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "                    # main confusion matrix\n",
    "                    cm = confusion_matrix(y21, y_pred1)\n",
    "                    cm_per_class = multilabel_confusion_matrix(y21, y_pred1)\n",
    "                    # Overall Accuracy\n",
    "                    Overall_Accuracy = np.sum(np.diagonal(cm)) / np.sum(cm)\n",
    "                    Overall_Accuracy = round(Overall_Accuracy*100, 2)\n",
    "                    # create confusion matrix table (pd.DataFrame)\n",
    "                    # cm_table = pd.DataFrame(cm, index=categories , columns=categories)\n",
    "                    if (i+1)!=1:\n",
    "                      feature_no= 'top_'+str(i+1)+'_features'\n",
    "                    else:\n",
    "                      feature_no= 'top_'+str(i+1)+'_feature'\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        Eval_Mat = []\n",
    "                        # per class metricies\n",
    "                        for i in range(len(categories)):\n",
    "                            TN = cm_per_class[i][0][0]\n",
    "                            FP = cm_per_class[i][0][1]\n",
    "                            FN = cm_per_class[i][1][0]\n",
    "                            TP = cm_per_class[i][1][1]\n",
    "                            Accuracy = round(100*(TP+TN)/(TP+TN+FP+FN), 2)\n",
    "                            Precision = round(100*(TP)/(TP+FP), 2)\n",
    "                            Sensitivity = round(100*(TP)/(TP+FN), 2)\n",
    "                            F1_score = round((2*Precision*Sensitivity)/(Precision+Sensitivity), 2)\n",
    "                            Specificity = round(100*(TN)/(TN+FP), 2)\n",
    "                            Eval_Mat.append([Accuracy, Precision, Sensitivity, F1_score, Specificity])\n",
    "                        # sizes of each class\n",
    "                        s2 = np.sum(cm,axis=1)\n",
    "                        # create tmep excel table\n",
    "                        headers=['Accuracy', 'Precision', 'Sensitivity', 'F1_score', 'Specificity']\n",
    "                        temp_table = pd.DataFrame(Eval_Mat, index=categories ,columns=headers)\n",
    "                        # weighted average of per class metricies\n",
    "                        ac=Overall_Accuracy\n",
    "                        # ac = round(temp_table['Accuracy'].dot(s)/np.sum(s), 2)\n",
    "                        pr = round(temp_table['Precision'].dot(s2)/np.sum(s2), 2)\n",
    "                        rc = round(temp_table['Sensitivity'].dot(s2)/np.sum(s2), 2)\n",
    "                        f1 = round(temp_table['F1_score'].dot(s2)/np.sum(s2), 2)\n",
    "                        sp = round(temp_table['Specificity'].dot(s2)/np.sum(s2), 2)\n",
    "\n",
    "\n",
    "                    except:\n",
    "                        ac='NaN'\n",
    "                        # ac = round(temp_table['Accuracy'].dot(s)/np.sum(s), 2)\n",
    "                        pr = 'NaN'\n",
    "                        rc ='NaN'\n",
    "                        f1 = 'NaN'\n",
    "                        sp = 'NaN'\n",
    "\n",
    "                    a.append(ac)\n",
    "                    p.append(pr)\n",
    "                    r.append(rc)\n",
    "                    s.append(sp)\n",
    "                    f.append(f1)\n",
    "                    feat.append(feature_no)\n",
    "\n",
    "\n",
    "#                     conf_matrix =confusion_matrix(y2, y_pred)\n",
    "\n",
    "#                     print('************** ')\n",
    "#                     print(\"Top %d  feature\" %(i+1))\n",
    "#                     print('************** ')\n",
    "#                     print(conf_matrix)\n",
    "#\n",
    "\n",
    "\n",
    "                Result=pd.concat([pd.DataFrame(a),pd.DataFrame(p),pd.DataFrame(r),pd.DataFrame(s),pd.DataFrame(f)],1)\n",
    "                Result.columns=['Accuracy','Precision','Recall','Specificity','F1-score']\n",
    "                Result.index= feat\n",
    "                #Result.to_csv\n",
    "                print('---------------------------------------------------------------------')\n",
    "                print('Result for '+clff[l]+' classifier')\n",
    "                print('---------------------------------------------------------------------')\n",
    "                print(Result)\n",
    "    #             Result.to_csv('/content/'+clff[l]+'_classifier_for_top10_features.csv')\n",
    "#                 l=l+1\n",
    "                print('---------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "                return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def classification_with_top_feature(data,feature_num,feature_selection_model,classifier,feat_increment):\n",
    "\n",
    "        xtrain,xtest,ytrain,ytest=data['data']\n",
    "        ind=data['index'].to_list()\n",
    "        num_feat=feature_num\n",
    "        fsm=feature_selection_model\n",
    "        feature=fsm[0:num_feat]\n",
    "        clf,clff=models()\n",
    "\n",
    "\n",
    "        if classifier=='all':\n",
    "            l=0\n",
    "            for c in range(21):\n",
    "\n",
    "                clf1=clf[c]\n",
    "                a=[]\n",
    "                p=[]\n",
    "                r=[]\n",
    "                s=[]\n",
    "                f=[]\n",
    "                mean_tpr=[]\n",
    "                mean_auc=[]\n",
    "\n",
    "                feat=[]\n",
    "                for i in list(range(0,num_feat,feat_increment)):\n",
    "\n",
    "                    y_pred=[]\n",
    "                    y2=[]\n",
    "                    tl=fsm[0:i+1]  #feature increasing\n",
    "                    tprs = []\n",
    "                    aucs = []\n",
    "                    mean_fpr = np.linspace(0,1,100)\n",
    "\n",
    "                    total_fold_num = len(xtrain)\n",
    "                    for k in range(total_fold_num):\n",
    "                        x11=pd.DataFrame(xtrain[k])\n",
    "                        x11.columns=ind\n",
    "                        x1=x11[tl]\n",
    "                        y1=ytrain[k]\n",
    "                        model = clf1.fit(np.array(x1),np.array(y1))\n",
    "                        #model = clf1.fit(x[train],y.iloc[train])\n",
    "                        xts=pd.DataFrame(xtest[k])\n",
    "                        xts.columns=ind\n",
    "                        xt1=xts[tl]\n",
    "                        y_pr=model.predict(xt1)\n",
    "\n",
    "\n",
    "                        y_pred.extend(y_pr)\n",
    "                        y2.extend(ytest[k])\n",
    "\n",
    "\n",
    "\n",
    "                    y21=y2\n",
    "                    y_pred1=y_pred\n",
    "                    categories=list(pd.Series(y2).unique())\n",
    "\n",
    "\n",
    "\n",
    "                    from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "                    # main confusion matrix\n",
    "                    cm = confusion_matrix(y21, y_pred1)\n",
    "                    # cm_per_class: it returns a 2x2 confusion matrix for each class, where 'i' represnt  class index\n",
    "                    # cm_per_class[i][0][0]:TN,   cm_per_class[i][0][1]:FP,   cm_per_class[i][1][0]:FN,    cm_per_class[i][1][1]:TP\n",
    "                    cm_per_class = multilabel_confusion_matrix(y21, y_pred1)\n",
    "                    # Overall Accuracy\n",
    "                    Overall_Accuracy = np.sum(np.diagonal(cm)) / np.sum(cm)\n",
    "                    Overall_Accuracy = round(Overall_Accuracy*100, 2)\n",
    "                    # create confusion matrix table (pd.DataFrame)\n",
    "                    # cm_table = pd.DataFrame(cm, index=categories , columns=categories)\n",
    "                    if (i+1)!=1:\n",
    "                      feature_no= 'top_'+str(i+1)+'_features'\n",
    "                    else:\n",
    "                      feature_no= 'top_'+str(i+1)+'_feature'\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        Eval_Mat = []\n",
    "                        # per class metricies\n",
    "                        for i in range(len(categories)):\n",
    "                            TN = cm_per_class[i][0][0]\n",
    "                            FP = cm_per_class[i][0][1]\n",
    "                            FN = cm_per_class[i][1][0]\n",
    "                            TP = cm_per_class[i][1][1]\n",
    "                            Accuracy = round(100*(TP+TN)/(TP+TN+FP+FN), 2)\n",
    "                            Precision = round(100*(TP)/(TP+FP), 2)\n",
    "                            Sensitivity = round(100*(TP)/(TP+FN), 2)\n",
    "                            F1_score = round((2*Precision*Sensitivity)/(Precision+Sensitivity), 2)\n",
    "                            Specificity = round(100*(TN)/(TN+FP), 2)\n",
    "                            Eval_Mat.append([Accuracy, Precision, Sensitivity, F1_score, Specificity])\n",
    "                        # sizes of each class\n",
    "                        s2 = np.sum(cm,axis=1)\n",
    "                        # create tmep excel table\n",
    "                        headers=['Accuracy', 'Precision', 'Sensitivity', 'F1_score', 'Specificity']\n",
    "                        temp_table = pd.DataFrame(Eval_Mat, index=categories ,columns=headers)\n",
    "                        # weighted average of per class metricies\n",
    "                        ac=Overall_Accuracy\n",
    "                        # ac = round(temp_table['Accuracy'].dot(s)/np.sum(s), 2)\n",
    "                        pr = round(temp_table['Precision'].dot(s2)/np.sum(s2), 2)\n",
    "                        rc = round(temp_table['Sensitivity'].dot(s2)/np.sum(s2), 2)\n",
    "                        f1 = round(temp_table['F1_score'].dot(s2)/np.sum(s2), 2)\n",
    "                        sp = round(temp_table['Specificity'].dot(s2)/np.sum(s2), 2)\n",
    "\n",
    "\n",
    "                    except:\n",
    "                        ac='NaN'\n",
    "                        # ac = round(temp_table['Accuracy'].dot(s)/np.sum(s), 2)\n",
    "                        pr = 'NaN'\n",
    "                        rc ='NaN'\n",
    "                        f1 = 'NaN'\n",
    "                        sp = 'NaN'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    a.append(ac)\n",
    "                    p.append(pr)\n",
    "                    r.append(rc)\n",
    "                    s.append(sp)\n",
    "                    f.append(f1)\n",
    "                    feat.append(feature_no)\n",
    "\n",
    "\n",
    "\n",
    "                Result=pd.concat([pd.DataFrame(a),pd.DataFrame(p),pd.DataFrame(r),pd.DataFrame(s),pd.DataFrame(f)],1)\n",
    "                Result.columns=['Accuracy','Precision','Recall','Specificity','F1-score']\n",
    "                Result.index= feat\n",
    "                #Result.to_csv\n",
    "                print('---------------------------------------------------------------------')\n",
    "                print('Result for '+clff[l]+' classifier')\n",
    "                print('---------------------------------------------------------------------')\n",
    "                print(Result)\n",
    "    #             Result.to_csv('/content/'+clff[l]+'_classifier_for_top10_features.csv')\n",
    "\n",
    "                print('---------------------------------------------------------------------')\n",
    "                l=l+1\n",
    "\n",
    "            return\n",
    "        else:\n",
    "\n",
    "                clf1=clf[classifier]  #model 0= MLP, 1= LDA, 2 = XGBoost, 3 = RF, 4= Logit, 5=SVC, 6 = Extra tree, 7= Adaboost, 8 = KNN, 9 = GradientBoost\n",
    "                l=classifier\n",
    "#             l=0\n",
    "\n",
    "\n",
    "#                 clf1=clf[c]\n",
    "                a=[]\n",
    "                p=[]\n",
    "                r=[]\n",
    "                s=[]\n",
    "                f=[]\n",
    "                mean_tpr=[]\n",
    "                mean_auc=[]\n",
    "\n",
    "                feat=[]\n",
    "                for i in list(range(0,num_feat,feat_increment)):\n",
    "\n",
    "                    y_pred=[]\n",
    "                    y2=[]\n",
    "                    tl=fsm[0:i+1]\n",
    "                    tprs = []\n",
    "                    aucs = []\n",
    "                    mean_fpr = np.linspace(0,1,100)\n",
    "\n",
    "                    total_fold_num = len(xtrain)\n",
    "                    for k in range(total_fold_num):\n",
    "                        x11=pd.DataFrame(xtrain[k])\n",
    "                        x11.columns=ind\n",
    "                        x1=x11[tl]\n",
    "                        y1=ytrain[k]\n",
    "                        model = clf1.fit(np.array(x1),np.array(y1))\n",
    "                        #model = clf1.fit(x[train],y.iloc[train])\n",
    "                        xts=pd.DataFrame(xtest[k])\n",
    "                        xts.columns=ind\n",
    "                        xt1=xts[tl]\n",
    "                        y_pr=model.predict(np.array(xt1))\n",
    "\n",
    "                        y_pred.extend(y_pr)\n",
    "                        y2.extend(ytest[k])\n",
    "\n",
    "\n",
    "\n",
    "                    y21=y2\n",
    "                    y_pred1=y_pred\n",
    "\n",
    "                    categories=list(pd.Series(y2).unique())\n",
    "                    from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "                    # main confusion matrix\n",
    "                    cm = confusion_matrix(y21, y_pred1)\n",
    "                    cm_per_class = multilabel_confusion_matrix(y21, y_pred1)\n",
    "                    # Overall Accuracy\n",
    "                    Overall_Accuracy = np.sum(np.diagonal(cm)) / np.sum(cm)\n",
    "                    Overall_Accuracy = round(Overall_Accuracy*100, 2)\n",
    "                    # create confusion matrix table (pd.DataFrame)\n",
    "                    # cm_table = pd.DataFrame(cm, index=categories , columns=categories)\n",
    "                    if (i+1)!=1:\n",
    "                      feature_no= 'top_'+str(i+1)+'_features'\n",
    "                    else:\n",
    "                      feature_no= 'top_'+str(i+1)+'_feature'\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        Eval_Mat = []\n",
    "                        # per class metricies\n",
    "                        for i in range(len(categories)):\n",
    "                            TN = cm_per_class[i][0][0]\n",
    "                            FP = cm_per_class[i][0][1]\n",
    "                            FN = cm_per_class[i][1][0]\n",
    "                            TP = cm_per_class[i][1][1]\n",
    "                            Accuracy = round(100*(TP+TN)/(TP+TN+FP+FN), 2)\n",
    "                            Precision = round(100*(TP)/(TP+FP), 2)\n",
    "                            Sensitivity = round(100*(TP)/(TP+FN), 2)\n",
    "                            F1_score = round((2*Precision*Sensitivity)/(Precision+Sensitivity), 2)\n",
    "                            Specificity = round(100*(TN)/(TN+FP), 2)\n",
    "                            Eval_Mat.append([Accuracy, Precision, Sensitivity, F1_score, Specificity])\n",
    "                        # sizes of each class\n",
    "                        s2 = np.sum(cm,axis=1)\n",
    "                        # create tmep excel table\n",
    "                        headers=['Accuracy', 'Precision', 'Sensitivity', 'F1_score', 'Specificity']\n",
    "                        temp_table = pd.DataFrame(Eval_Mat, index=categories ,columns=headers)\n",
    "                        # weighted average of per class metricies\n",
    "                        ac=Overall_Accuracy\n",
    "                        # ac = round(temp_table['Accuracy'].dot(s)/np.sum(s), 2)\n",
    "                        pr = round(temp_table['Precision'].dot(s2)/np.sum(s2), 2)\n",
    "                        rc = round(temp_table['Sensitivity'].dot(s2)/np.sum(s2), 2)\n",
    "                        f1 = round(temp_table['F1_score'].dot(s2)/np.sum(s2), 2)\n",
    "                        sp = round(temp_table['Specificity'].dot(s2)/np.sum(s2), 2)\n",
    "\n",
    "\n",
    "                    except:\n",
    "                        ac='NaN'\n",
    "                        # ac = round(temp_table['Accuracy'].dot(s)/np.sum(s), 2)\n",
    "                        pr = 'NaN'\n",
    "                        rc ='NaN'\n",
    "                        f1 = 'NaN'\n",
    "                        sp = 'NaN'\n",
    "\n",
    "                    a.append(ac)\n",
    "                    p.append(pr)\n",
    "                    r.append(rc)\n",
    "                    s.append(sp)\n",
    "                    f.append(f1)\n",
    "                    feat.append(feature_no)\n",
    "\n",
    "\n",
    "#                     conf_matrix =confusion_matrix(y2, y_pred)\n",
    "\n",
    "#                     print('************** ')\n",
    "#                     print(\"Top %d  feature\" %(i+1))\n",
    "#                     print('************** ')\n",
    "#                     print(conf_matrix)\n",
    "#\n",
    "\n",
    "\n",
    "                Result=pd.concat([pd.DataFrame(a),pd.DataFrame(p),pd.DataFrame(r),pd.DataFrame(s),pd.DataFrame(f)],1)\n",
    "                Result.columns=['Accuracy','Precision','Recall','Specificity','F1-score']\n",
    "                Result.index= feat\n",
    "                #Result.to_csv\n",
    "                print('---------------------------------------------------------------------')\n",
    "                print('Result for '+clff[l]+' classifier')\n",
    "                print('---------------------------------------------------------------------')\n",
    "                print(Result)\n",
    "    #             Result.to_csv('/content/'+clff[l]+'_classifier_for_top10_features.csv')\n",
    "#                 l=l+1\n",
    "                print('---------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "                return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def classification_with_combined_features(data,feature_num,feature_selection_model,classifier):\n",
    "\n",
    "    xtrain,xtest,ytrain,ytest=data['data']\n",
    "    ind=data['index'].to_list()\n",
    "    num_feat=feature_num\n",
    "    fsm=feature_selection_model\n",
    "    # feature=fsm[0:num_feat]\n",
    "    clf,clff=models()\n",
    "    classifier='all'\n",
    "\n",
    "    if classifier=='all':\n",
    "        l=0\n",
    "        a=[]\n",
    "        p=[]\n",
    "        r=[]\n",
    "        s=[]\n",
    "        f=[]\n",
    "        prb0=[]\n",
    "        prb1=[]\n",
    "        pred=[]\n",
    "        tar=[]\n",
    "\n",
    "        for c in range(21):\n",
    "\n",
    "            clf1=clf[c]\n",
    "\n",
    "            feat=[]\n",
    "            for i in list(range(1)):\n",
    "\n",
    "                y_pred=[]\n",
    "                y2=[]\n",
    "                tl=fsm[0:num_feat]\n",
    "                probs=[]\n",
    "                probss=[]\n",
    "\n",
    "                total_fold_num = len(xtrain)\n",
    "                for k in range(total_fold_num):\n",
    "                    x11=pd.DataFrame(xtrain[k])\n",
    "                    x11.columns=ind\n",
    "                    x1=x11[tl]\n",
    "                    y1=ytrain[k]\n",
    "                    model = clf1.fit(np.array(x1),np.array(y1))\n",
    "                    #model = clf1.fit(x[train],y.iloc[train])\n",
    "                    xts=pd.DataFrame(xtest[k])\n",
    "                    xts.columns=ind\n",
    "                    xt1=xts[tl]\n",
    "                    y_pr=model.predict(np.array(xt1))\n",
    "                    y_prob=model.predict_proba(np.array(xt1))\n",
    "                    y_pred.extend(y_pr)\n",
    "                    y2.extend(ytest[k])\n",
    "                    probs.extend(y_prob)\n",
    "                    probss.append(y_prob)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                categories=list(pd.Series(y2).unique())\n",
    "                y21, y_pred1=y2,y_pred\n",
    "                if (i+1)!=1:\n",
    "                  feature_no= 'top_'+str(i+1)+'_features'\n",
    "                else:\n",
    "                  feature_no= 'top_'+str(i+1)+'_feature'\n",
    "\n",
    "\n",
    "                from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "                # main confusion matrix\n",
    "                cm = confusion_matrix(y21, y_pred1)\n",
    "\n",
    "\n",
    "                cm_per_class = multilabel_confusion_matrix(y21, y_pred1)\n",
    "                # Overall Accuracy\n",
    "                Overall_Accuracy = np.sum(np.diagonal(cm)) / np.sum(cm)\n",
    "                Overall_Accuracy = round(Overall_Accuracy*100, 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                Eval_Mat = []\n",
    "                # per class metricies\n",
    "                for i in range(len(categories)):\n",
    "                    TN = cm_per_class[i][0][0]\n",
    "                    FP = cm_per_class[i][0][1]\n",
    "                    FN = cm_per_class[i][1][0]\n",
    "                    TP = cm_per_class[i][1][1]\n",
    "                    Accuracy = round(100*(TP+TN)/(TP+TN+FP+FN), 2)\n",
    "                    Precision = round(100*(TP)/(TP+FP), 2)\n",
    "                    Sensitivity = round(100*(TP)/(TP+FN), 2)\n",
    "                    F1_score = round((2*Precision*Sensitivity)/(Precision+Sensitivity), 2)\n",
    "                    Specificity = round(100*(TN)/(TN+FP), 2)\n",
    "                    Eval_Mat.append([Accuracy, Precision, Sensitivity, F1_score, Specificity])\n",
    "                # sizes of each class\n",
    "                s2 = np.sum(cm,axis=1)\n",
    "                # create tmep excel table\n",
    "                headers=['Accuracy', 'Precision', 'Sensitivity', 'F1_score', 'Specificity']\n",
    "                temp_table = pd.DataFrame(Eval_Mat, index=categories ,columns=headers)\n",
    "                # weighted average of per class metricies\n",
    "                ac=Overall_Accuracy\n",
    "                # ac = round(temp_table['Accuracy'].dot(s)/np.sum(s), 2)\n",
    "                pr = round(temp_table['Precision'].dot(s2)/np.sum(s2), 2)\n",
    "                rc = round(temp_table['Sensitivity'].dot(s2)/np.sum(s2), 2)\n",
    "                f1 = round(temp_table['F1_score'].dot(s2)/np.sum(s2), 2)\n",
    "                sp = round(temp_table['Specificity'].dot(s2)/np.sum(s2), 2)\n",
    "                a.append(ac)\n",
    "                p.append(pr)\n",
    "                r.append(rc)\n",
    "                s.append(sp)\n",
    "                f.append(f1)\n",
    "                feat.append(feature_no)\n",
    "                prb0.append(probs)\n",
    "                prb1.append(probss)\n",
    "                pred.append(y_pred1)\n",
    "                tar.append(y2)\n",
    "\n",
    "\n",
    "        Result=pd.concat([pd.DataFrame(a),pd.DataFrame(p),pd.DataFrame(r),pd.DataFrame(s),pd.DataFrame(f)],1)\n",
    "        Result.columns=['Accuracy','Precision','Recall','Specificity','F1-score']\n",
    "        Result.index= clff\n",
    "\n",
    "        print(Result)\n",
    "\n",
    "        l=l+1\n",
    "        print('---------------------------------------------------------------------')\n",
    "        return  Result, prb1,prb0,ytest,tar,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhLAB1M4uuZj"
   },
   "outputs": [],
   "source": [
    "feature_num=17\n",
    "feat_increment=1  #increment feature numbers\n",
    "feature_selection_model = extratree  ##choose any of them [xgboost,randomforest,extratree,genetic_feature_selection]\n",
    "classifier= 4\n",
    "\n",
    "\n",
    "#Classification with top features increasing by 1 by 1\n",
    "classification_with_top_feature_v_2(fold_data,feature_num,feature_selection_model,classifier,feat_increment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeyWMBDcvKUI"
   },
   "outputs": [],
   "source": [
    "feature_num= 7\n",
    "feature_selection_model= extratree  ##choose any of them [xgboost,randomforest,extratree, genetic_feature_selection]\n",
    "classifier='all' ## choose 'all' for all models\n",
    "\n",
    "#Classification with top features increasing by 1 by 1\n",
    "Result, prb1,prb0,ytest,tar,pred=classification_with_combined_features(fold_data,feature_num,feature_selection_model,classifier)\n",
    "display(Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8r0CjY3kvKaj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndqZAJCivaUK"
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzUilKLQvcob"
   },
   "outputs": [],
   "source": [
    "def processed_data(ml1,ml2,ml3,td2):\n",
    "    xts=[]\n",
    "    xtr=[]\n",
    "    yts=[]\n",
    "    ytr=[]\n",
    "\n",
    "\n",
    "    prf=[]\n",
    "    for i in range(5):\n",
    "      pl=np.concatenate((ml1[i],ml2[i],ml3[i]),1)\n",
    "      prf.append(pl)\n",
    "\n",
    "    for j in range(5):\n",
    "      if j==1:\n",
    "        yts.append(td2[j])\n",
    "        ytr.append(np.concatenate((td2[2],td2[3],td2[4],td2[0]),0))\n",
    "        xts.append(prf[j])\n",
    "        xtr.append(np.concatenate((prf[2],prf[3],prf[4],prf[0]),0))\n",
    "      elif j==2:\n",
    "        yts.append(td2[j])\n",
    "        ytr.append(np.concatenate((td2[1],td2[3],td2[4],td2[0]),0))\n",
    "        xts.append(prf[j])\n",
    "        xtr.append(np.concatenate((prf[1],prf[3],prf[4],prf[0]),0))\n",
    "      elif j==3:\n",
    "        yts.append(td2[j])\n",
    "        ytr.append(np.concatenate((td2[1],td2[2],td2[4],td2[0]),0))\n",
    "        xts.append(prf[j])\n",
    "        xtr.append(np.concatenate((prf[1],prf[2],prf[4],prf[0]),0))\n",
    "      elif j==4:\n",
    "        yts.append(td2[j])\n",
    "        ytr.append(np.concatenate((td2[1],td2[3],td2[2],td2[0]),0))\n",
    "        xts.append(prf[j])\n",
    "        xtr.append(np.concatenate((prf[1],prf[3],prf[2],prf[0]),0))\n",
    "      elif j==0:\n",
    "        yts.append(td2[j])\n",
    "        ytr.append(np.concatenate((td2[1],td2[3],td2[2],td2[4]),0))\n",
    "        xts.append(prf[j])\n",
    "        xtr.append(np.concatenate((prf[1],prf[3],prf[2],prf[4]),0))\n",
    "    return xtr,xts,ytr,yts\n",
    "\n",
    "\n",
    "\n",
    "def stacking_classification(ml1,ml2,ml3,td2):\n",
    "\n",
    "    xtrain,xtest,ytrain,ytest=processed_data(ml1,ml2,ml3,td2)\n",
    "    clf,clff=models()\n",
    "    classifier='all'\n",
    "    if classifier=='all':\n",
    "      l=0\n",
    "      a=[]\n",
    "      p=[]\n",
    "      r=[]\n",
    "      s=[]\n",
    "      f=[]\n",
    "      prb0=[]\n",
    "      prb1=[]\n",
    "      pred=[]\n",
    "      tar=[]\n",
    "\n",
    "      for c in range(21):\n",
    "\n",
    "          clf1=clf[c]\n",
    "\n",
    "          feat=[]\n",
    "          for i in list(range(1)):\n",
    "\n",
    "              y_pred=[]\n",
    "              y2=[]\n",
    "              # tl=fsm[0:num_feat]\n",
    "              probs=[]\n",
    "              probss=[]\n",
    "\n",
    "              total_fold_num = len(xtrain)\n",
    "              for k in range(total_fold_num):\n",
    "                  x1=pd.DataFrame(xtrain[k])\n",
    "                  # x11.columns=ind\n",
    "                  # x1=x11[tl]\n",
    "                  y1=ytrain[k]\n",
    "                  model = clf1.fit(np.array(x1),np.array(y1))\n",
    "                  #model = clf1.fit(x[train],y.iloc[train])\n",
    "                  xt1=pd.DataFrame(xtest[k])\n",
    "                  # xts.columns=ind\n",
    "                  # xt1=xts[tl]\n",
    "                  y_pr=model.predict(np.array(xt1))\n",
    "                  y_prob=model.predict_proba(np.array(xt1))\n",
    "                  y_pred.extend(y_pr)\n",
    "                  y2.extend(ytest[k])\n",
    "                  probs.extend(y_prob)\n",
    "                  probss.append(y_prob)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "              categories=list(pd.Series(y2).unique())\n",
    "              y21, y_pred1=y2,y_pred\n",
    "              if (i+1)!=1:\n",
    "                feature_no= 'top_'+str(i+1)+'_features'\n",
    "              else:\n",
    "                feature_no= 'top_'+str(i+1)+'_feature'\n",
    "\n",
    "\n",
    "              from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "              # main confusion matrix\n",
    "              cm = confusion_matrix(y21, y_pred1)\n",
    "              # cm_per_class: it returns a 2x2 confusion matrix for each class, where 'i' represnt  class index\n",
    "              # cm_per_class[i][0][0]:TN,   cm_per_class[i][0][1]:FP,   cm_per_class[i][1][0]:FN,    cm_per_class[i][1][1]:TP\n",
    "              cm_per_class = multilabel_confusion_matrix(y21, y_pred1)\n",
    "              # Overall Accuracy\n",
    "              Overall_Accuracy = np.sum(np.diagonal(cm)) / np.sum(cm)\n",
    "              Overall_Accuracy = round(Overall_Accuracy*100, 2)\n",
    "\n",
    "              Eval_Mat = []\n",
    "              # per class metricies\n",
    "              for i in range(len(categories)):\n",
    "                  TN = cm_per_class[i][0][0]\n",
    "                  FP = cm_per_class[i][0][1]\n",
    "                  FN = cm_per_class[i][1][0]\n",
    "                  TP = cm_per_class[i][1][1]\n",
    "                  Accuracy = round(100*(TP+TN)/(TP+TN+FP+FN), 2)\n",
    "                  Precision = round(100*(TP)/(TP+FP), 2)\n",
    "                  Sensitivity = round(100*(TP)/(TP+FN), 2)\n",
    "                  F1_score = round((2*Precision*Sensitivity)/(Precision+Sensitivity), 2)\n",
    "                  Specificity = round(100*(TN)/(TN+FP), 2)\n",
    "                  Eval_Mat.append([Accuracy, Precision, Sensitivity, F1_score, Specificity])\n",
    "              # sizes of each class\n",
    "              s2 = np.sum(cm,axis=1)\n",
    "              # create tmep excel table\n",
    "              headers=['Accuracy', 'Precision', 'Sensitivity', 'F1_score', 'Specificity']\n",
    "              temp_table = pd.DataFrame(Eval_Mat, index=categories ,columns=headers)\n",
    "              # weighted average of per class metricies\n",
    "              ac=Overall_Accuracy\n",
    "              # ac = round(temp_table['Accuracy'].dot(s)/np.sum(s), 2)\n",
    "              pr = round(temp_table['Precision'].dot(s2)/np.sum(s2), 2)\n",
    "              rc = round(temp_table['Sensitivity'].dot(s2)/np.sum(s2), 2)\n",
    "              f1 = round(temp_table['F1_score'].dot(s2)/np.sum(s2), 2)\n",
    "              sp = round(temp_table['Specificity'].dot(s2)/np.sum(s2), 2)\n",
    "              a.append(ac)\n",
    "              p.append(pr)\n",
    "              r.append(rc)\n",
    "              s.append(sp)\n",
    "              f.append(f1)\n",
    "              feat.append(feature_no)\n",
    "              prb0.append(probs)\n",
    "              prb1.append(probss)\n",
    "              pred.append(y_pred1)\n",
    "              tar.append(y2)\n",
    "\n",
    "      Result=pd.concat([pd.DataFrame(a),pd.DataFrame(p),pd.DataFrame(r),pd.DataFrame(s),pd.DataFrame(f)],1)\n",
    "      Result.columns=['Accuracy','Precision','Recall','Specificity','F1-score']\n",
    "      Result.index= clff\n",
    "\n",
    "      print(Result)\n",
    "\n",
    "      l=l+1\n",
    "      print('---------------------------------------------------------------------')\n",
    "      return  Result, prb1,prb0,ytest,tar,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDwI9tKbvsXz"
   },
   "outputs": [],
   "source": [
    "ml1=prb1[7] #model 1\n",
    "\n",
    "ml2=prb1[11]  #model 2\n",
    "\n",
    "ml3=prb1[16]  #model 3\n",
    "\n",
    "td2=ytest  # target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hu98wWkFvx26"
   },
   "outputs": [],
   "source": [
    "Result2, prb2,prb02,ytest2,tar2,pred2=stacking_classification(ml1,ml2,ml3,td2)\n",
    "Result2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vDDDQDM7nyz_",
    "YsBGAbLuosWS",
    "A9HpWSFTsnnA",
    "qt7dQc1rtDTA",
    "-fAwzmESthTJ",
    "1w4LhtWMuBYx",
    "JhH3sch9uQtg",
    "ndqZAJCivaUK"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
